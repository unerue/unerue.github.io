{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8599890",
   "metadata": {},
   "source": [
    "# Image Denoising with Denoising Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5896dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "#! pip install torchviz\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchviz import make_dot\n",
    "\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "#print(torch.cuda.current_device())\n",
    "#print(torch.cuda.device(0))\n",
    "#print(torch.cuda.device_count())\n",
    "#print(torch.cuda.get_device_name(0))\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.view(x.size(0), 1, 50, 37)\n",
    "    return x\n",
    "\n",
    "num_epochs = 100 #100\n",
    "batch_size = 8 # 16\n",
    "learning_rate = 1e-3\n",
    "cuda = False #True \n",
    "\n",
    "\n",
    "def add_noise(img):\n",
    "    noise = torch.randn(img.size()) * 0.2\n",
    "    noisy_img = img + noise\n",
    "    return noisy_img\n",
    "\n",
    "\n",
    "def plot_sample_img(img, name):\n",
    "    img = img.view(1, 50, 37)\n",
    "    save_image(img, './sample_{}.png'.format(name))\n",
    "\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def tensor_round(tensor):\n",
    "    return torch.round(tensor)\n",
    "\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda tensor:min_max_normalization(tensor, 0, 1)),\n",
    "    transforms.Lambda(lambda tensor:tensor_round(tensor))\n",
    "])\n",
    "\n",
    "dataset = fetch_lfw_people(min_faces_per_person=70, resize=0.4).images / 255\n",
    "print(dataset.shape)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(50 * 37, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 50 * 37),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = autoencoder()\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for data in dataloader:\n",
    "        img = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        noisy_img = add_noise(img)\n",
    "        noisy_img = Variable(noisy_img)\n",
    "        if cuda:\n",
    "            noisy_img = noisy_img.cuda()\n",
    "        img = Variable(img)\n",
    "        if cuda:\n",
    "            img = img.cuda()\n",
    "        # ===================forward=====================\n",
    "        output = model(noisy_img)\n",
    "        loss = criterion(output, img)\n",
    "        MSE_loss = nn.MSELoss()(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'\n",
    "          .format(epoch, num_epochs, loss.data.item(), MSE_loss.data.item()))\n",
    "    if epoch % 10 == 0:\n",
    "        x = to_img(img.cpu().data)\n",
    "        x_hat = to_img(output.cpu().data)\n",
    "        x_noisy = to_img(noisy_img.cpu().data)\n",
    "        weights = to_img(model.encoder[0].weight.cpu().data)\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.gray()\n",
    "        for i in range(8):\n",
    "            plt.subplot(8,8,i+1), plt.imshow(x.data.numpy()[i,0,...]), plt.axis('off')\n",
    "        for i in range(8):\n",
    "            plt.subplot(8,8,i+9), plt.imshow(x_noisy.data.numpy()[i,0,...]), plt.axis('off')\n",
    "        for i in range(8):\n",
    "            plt.subplot(8,8,i+17), plt.imshow(x_hat.data.numpy()[i,0,...]), plt.axis('off')\n",
    "        indices = np.random.choice(512, 40)\n",
    "        for i in range(40):\n",
    "            plt.subplot(8,8,i+25), plt.imshow(weights.data.numpy()[indices[i],0,...]), plt.axis('off')\n",
    "        plt.suptitle('Original (Row 1), Noisy input (Row 2), DAE output (Row 3) images \\n and some features (Rows 4-8) learnt by the DAE in Epoch {}'.format(epoch), size=30)\n",
    "        plt.show()\n",
    "        #print(weights.shape)\n",
    "\n",
    "#torch.save(model.state_dict(), './sim_dautoencoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950e2957",
   "metadata": {},
   "source": [
    "## Image Denoising with Principal Component Analysis (PCA), Discrete Fourier / Wavelet Tranform (FFT, DWT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd922db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn import decomposition\n",
    "from skimage.util import random_noise\n",
    "from skimage import img_as_float\n",
    "from time import time\n",
    "import scipy.fftpack as fp\n",
    "import pywt\n",
    "\n",
    "n_components = 50 # 256\n",
    "image_shape = (64, 64)\n",
    "rng = RandomState(0)\n",
    "\n",
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\n",
    "original = img_as_float(dataset.data)\n",
    "faces = original.copy()\n",
    "print(faces.shape)\n",
    "\n",
    "n_samples, n_features = faces.shape\n",
    "#mean_face = faces.mean(axis=0)\n",
    "#faces = faces - mean_face\n",
    "faces = random_noise(faces, var=0.005)\n",
    "\n",
    "estimator = decomposition.PCA(n_components=n_components, svd_solver='randomized', whiten=True)\n",
    "print(\"Extracting the top %d PCs...\" % (n_components))\n",
    "t0 = time()\n",
    "faces_recons = estimator.inverse_transform(estimator.fit_transform(faces)) #.T #+ mean_face #.T\n",
    "train_time = (time() - t0)\n",
    "print(\"done in %0.3fs\" % train_time)\n",
    "\n",
    "indices = np.random.choice(n_samples, 5, replace=False)\n",
    "plt.figure(figsize=(20,4))\n",
    "for i in range(len(indices)):\n",
    "    plt.subplot(1,5,i+1), plt.imshow(np.reshape(original[indices[i],:], image_shape)), plt.axis('off')\n",
    "plt.suptitle('Original', size=25)\n",
    "plt.show()\n",
    "\n",
    "#faces = faces + mean_face\n",
    "plt.figure(figsize=(20,4))\n",
    "for i in range(len(indices)):\n",
    "    plt.subplot(1,5,i+1), plt.imshow(np.reshape(faces[indices[i],:], image_shape)), plt.axis('off')\n",
    "plt.suptitle('Noisy', size=25)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "for i in range(len(indices)):\n",
    "    plt.subplot(1,5,i+1), plt.imshow(np.reshape(faces_recons[indices[i],:], image_shape)), plt.axis('off')\n",
    "plt.suptitle('PCA reconstruction with {} components (eigenfaces)'.format(n_components), size=25)\n",
    "plt.show()\n",
    "\n",
    "n_components = 30 \n",
    "plt.figure(figsize=(20,4))\n",
    "for i in range(len(indices)):\n",
    "    freq = fp.fftshift(fp.fft2((np.reshape(faces[indices[i],:], image_shape)).astype(float)))\n",
    "    freq[:freq.shape[0]//2 - n_components//2,:] = freq[freq.shape[0]//2 + n_components//2:,:] = 0\n",
    "    freq[:, :freq.shape[1]//2 - n_components//2] = freq[:, freq.shape[1]//2 + n_components//2:] = 0\n",
    "    plt.subplot(1,5,i+1), plt.imshow(fp.ifft2(fp.ifftshift(freq)).real), plt.axis('off')\n",
    "plt.suptitle('FFT LPF reconstruction with {} basis vectors'.format(n_components), size=25)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "wavelet = pywt.Wavelet('haar')\n",
    "for i in range(len(indices)):\n",
    "    wavelet_coeffs = pywt.wavedec2((np.reshape(faces[indices[i],:], image_shape)).astype(float), wavelet)\n",
    "    plt.subplot(1,5,i+1), plt.imshow(pywt.waverec2(wavelet_coeffs[:-1], wavelet)), plt.axis('off')\n",
    "plt.suptitle('Wavelet reconstruction with {} subbands'.format(len(wavelet_coeffs)-1), size=25)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
